* Week 1: Overview and Stream Ciphers
** What is Cryptography All About?
*** 2 core parts
1. Establish and exchange a secure key
2. Use it to communicate securely

*** Digital Signatures
*** Anonymous communication
MixNets are ways of encrypting traffic through multiple proxies

*** Anonymous Digital Cash without double spending
*** Secure multi-party computation
Theorem: any computation that can be done with a trusted authority
can also be done without, in a cryptographically secure way.

*** Privately outsourcing computation
*** Zero knowledge proof of knowledge
Let's say we have N = p * q, where p and q are prime
There's a way for Alice to prove to Bob that she knows p and q for
N, without telling Bob either p or q!

*** Three steps in cryptography:
1. Specify the threat model: what can an attacker do to attack, and
what is the goal in forging
2. Propose a construction
3. Proof that breaking the construction under the threat model will
solve an underlying hard problem.

*** History of Cryptography
The Code Breakers by David Kahn gives a great history of cryptography
from ancient times to today.

Symmetric Ciphers: both Alice (the encrypter) and Bob (the decrypter)
use the same key to their algorithm.

** Discrete Probability
The Union Bound: P(A U B) <= P(A) + P(B)

Events A and B are independent if P(A ^ B) = P(A) * P(B)

XOR Theorem: Let Y be a random variable distributed over {0,1}^n,
and let X be an independent uniform variable on {0,1}^n. Then Z = (Y
xor X) is a uniform random variable on {0,1}^n.

** Stream Ciphers
***  A Cipher is defined over (K, M, C) is a pair of "efficient" algorithms (E, D) where
- K is the set of all possible keys
- M is the set of all possible messages
- C is the set of all possible cipher texts
- D(k, E(k, m)) = m (consistency property)

E is often randomized, but D must always be deterministic to satisfy the
consistency constraint

*** The One Time Pad Cipher
c := E(k, m) = k xor m

Since xor is addition modulo 2, this very simple cipher satisfies the
consistency property.

Very fast encryption and decryption, BUT the keys are as long as the plaintext,
so it's inefficient.

Satisfies the proof of "perfect secrecy": there are no cipher-text-only attacks
possible

*** Pseudorandom Keys
Rather than using a long key, we can use a seed to a pseudorandom number
generator to generate a key, and share the seed securely.

We lose the definition of "perfect secrecy", and instead rely on the notion of
it being "unpredictable".

*** Attacks on Stream Ciphers
**** Multiple use
Note that the one time pad is a ONE TIME pad! If I see two encrypted messages
using the same pad, I can xor the ciphers to get m1 xor m2

- Project Venona (1941-1946) is a good example of a failure of this kind
- Same with WEP
- In client server architectures, we need to have one key for
client -> server requests, and one key for server -> client responses

**** Malleability
The OTP provides no integrity: attackers can intercept the cipher and xor it
with their own p, to modify the message.

Can essentially do sed substitutions on the stream, if we know the offset of
what we're trying to change.

Example: changing "From: Ben" to "From: Bob" in a message
where the attacker knows Ben will appear and the offset

**** CSS for DVD encryption
Based on a Linear Feedback Shift Register (LFSR)

Implemented in hardware, then badly broken :)

*** Salsa20
Modern secure stream cipher designed for both software and hardware
implementations.

*** PRGs
Let G:K -> {0,1}^n be a PRG

We define a number of statistical tests to determine if a binary string X "looks
random".

Given a statistical test A and a generator G, we define the =advantage= of a
truly random number over G as the probability that G passes the test -
probability that a truly random number passes.

An advantage close to 1 means that G is failing; close to 0 means that G is as
good as a truly random number w.r.t. that statistical test.

If the advantage for test A is significant for G, we say "A breaks G with
advantage %"

We say that =G:k -> {0,1}^n= is a secure PRG if for all efficient statistical
tests A, =Advantage[A, G] < epsilon= for some "negligible" epsilon.

There are no provably secure PRGs, unless P = NP.
A secure PRG is unpredictable.

Thm: If for all i in {0, n-1}, PRG G is unpredictable at position i, then G is a
secure PRG.

If a next-bit predictor cannot distinguish G from random then no statistical
test can.

*** Semantic Security for a one-time key
An adversary emits two messages, m1 and m2, which are encrypted with the
algorithm. If the adversary is able to guess which message comes out given the
encrypted text, the algorithm does not have semantic security.

This is a weaker definition than perfect security, because it requires the
adversary to have an efficient algorithm to crack.

* Week 2: Block Ciphers
Examples: 3DES and AES

** Building block ciphers
Use iteration:
1. The key is expanded into n keys
2. The message is encrypted by k "round functions" =R(k, m)= to produce the resulting cipher =c=

The specifications to a block cipher are the key expansion function, the round
function, the key size, and the round count.

** Pseudo-Random Function (PRF)
A Pseudo-Random Function (PRF) defined over =(K, X, Y)= is a function
=f:  K x Y -> Y= such that an efficient algorithm exists to evaluate
=F(k, x)=

A Pseudo-Random Permutation (PRP) defined over =(K, X)= is =E: K x X -> X= such
that:
- There exists an efficient deterministic algorithm to evaluated =E(k, x)=
- The function =E(k, *)= is one-to-one
- There exists an efficient inversion algorithm =D(k, y)=

Note that this is a subset of PRFs. 3DES and AES are PRPs

A PRF is /secure/ if an adversary cannot tell the difference from a truly random
PRF, with only some neglibigle advantage

Same test as before: Adversary -> input -> output -> can you tell the
difference?

That is, the challenger chooses either a truly random function f, or a PRF f
with a fixed key k, and then the adversary submits q queries interacting with
it, for as many q as he likes, and then must answer whether he was interacting
with a random F or a PRF.

** Data Encryption Standard (DES)
Block Cipher with a key length of 56-bits, block length of 64 bits, both of
which resulted in it being broken by 1997. Replaced around 2000 by AES.

16 round Feistel Network on 64-bit blocks (2 * 32)

*** Feistel Network
We map a 2n-bit input to a 2n-bit output.

=Ri = Fi(Ri-1) xor Li=, =Li = Ri-1 for i in [1..d]=, where =d= is the number of
rounds. Each =Li= goes on to feed =Ri+1= in the network, and vice versa.

The right side just passes through to the left side of the next level, while the
left side is applied to f_n(Right) and xor'd with the left side to produce
the next right side.

This network is invertible: the only difference between the encryption and
decryption circuits is the order in which the functions are applied.

Many block ciphers use feistel networks.

Thm: (Luby-Rackoff '85)
Let =f: K x {0,1}^n -> {0,1}^n= be a secure PRF. Then a 3-round Feistel
network =F: K^3 x {0,1}^2n -> {0,1}^2n= is a secure PRP.

*** How DES works
The function =F(k_i, x)= is defined on a 32-bit x and a 48-bit k_i.
There's a copying expansion that expands x to 48-bits, specified in the spec,
and then the 48-bit expanded X is xor'd with the 48-bit k_i.

We then break the result into 8 groups of 6 bits (S-boxes). Each S-box is a =f:
{0,1}^6 -> {0,1}^4= function that maps 6 bits to 4 bits, with a lookup table.

It is important to choose the S-Boxes carefully to avoid making the entire DES
function linear, which would let us specify it entirely in a matrix
multiplication.

We then run the output from the S-boxes through a 32-bit permutation function.

** Exhaustive Search Attacks
Goal: given input output pairs:
=(m_i, c_i = E(k, m_i))= for =i = 1,..3=, find the key =k=

Lemma: Suppose DES is an ideal cipher (made up of 2^56 random invertible functions).
Then for all m, c there is at most one key k such that =c = DES(k,m)=

*** 3DES
We define =3E: K^3 x M -> M= as =3E((k1,k2,k3), m) = E(k1, D(k2, E(k3,m)))=
That is, we encrypt with the 3rd key, decrypt with the 2nd, and encrypt with the
first. Notice if we set all 3 keys the same we end up with a DES implementation.

3DES increases the space size to protect against exhaustive search by moving the
key size up to 3x56 = 168 bits, at the cost of being 3x slower than DES.

"Double" DES was not proposed because it is vulnerable to "meet in the middle"
attacks.
Say we have =E(k1, E(k2, M)) = C=. Then this is equivalent to =E(k2, M) = D(k1,
C)=, by applying the decryption algorithm on each side.

We can build a sorted table of all 2^56 possible values =E(k2, M)= for a message m.
Then for each possible 2^56 key k1, we lookup to see if =D(k1, C)= is in the table,
and if they are then we've found k1 and k2.

*** DESX
Let =E: K x {0,1}^n -> {0,1}^n= be a block cipher.
Define EX as =EX((k1,k2,k3), m) = k1 xor E(k2, m xor k3)=

Doesn't have 3x performance penalty, but has more subtle security vulnerabilities.

** More sophisticated attacks
*** Side channel attacks
Measuring time to enc/dec, power for enc/dec precisely. This can expose the secret key k.
*** Fault attacks
Computing errors in the last round can expose the secrete key
*** Linear and differential attacks
If the message and cipher text are at all correlated with any relation, we can
determine some of the key bits.
*** Quantum attacks

** Advanced Encyrption Standard (AES) Block Cipher
*** Substitution-Permutation Network
Similar to Feistel Network, but all bits changed in every round (as opposed to
half).

Round: xor with round key, go through block substitution phase, permutation
layer, repeat. Fully reversible.

For AES-128, it operates on a 4 byte x 4 byte (= 16 bytes = 128 bits) block,
which is xor'd with each 16 byte round key, then run through invertible ByteSub,
ShiftRow, and MixColumn functions. This is repeated for 10 rounds.

Operates on a 128 bit (16 byte) key, which gets expanded to 11 x 16 = 176 bytes
for all the key expansion.

**** ByteSub
A 1-byte S-Box with a 256 byte table. Applies the S-Box to each element in the 4x4 input cell.
The S-Boxes are specified in such a way that they can be expanded via code or
hard-wired as a lookup table, to give implementation options for speed vs. memory use.

**** ShiftRows
Cyclic shift on each row.

**** MixColumns
Apply a linear transformation to each column, independently applied.

*** Code size vs. performance/security tradeoff
Pre-compute the round function tables vs. calculate them as-needed.

Intel/AMD have specific instructions in hardware for AES.

** Block Ciphers from PRGs
GGM PRF: Let =G: K -> K^2=. Define =PRF F: K x {0,1}^n -> K= as the sequence of
applying G to k and using either the left or right output depending on the ith
bit of k.

We can recursively continue this process to take any PRG =G: K -> K^2= to create
a PRF =F: K x {0,1}^n -> K= for any arbitrary n.

Namely, if we have a PRG that doubles its output, we can use it to define a PRF
of any arbitrary size.

Not used in practice due to slow performance, though thanks to the Luby Rackoff
Thm we can create an elegant PRP that's provably secure provided the underlying
PRG is secure.

*** Using Block Ciphers
PRF Switching Lemma: Any secure PRP is also a secure PRF, if |X| is sufficiently
large (~2^128).

Let E be a PRP over =(K, X)=. Then for any q-eury adversary A, =|Adv_prf[A,E]
-Adv_prp[A,E]| < q^2 / (2*|X|)=

Example with a 1-bit X size, we have a 1/2 advantage (huge).

**** One Time Key
***** Electronic Code Book
Terribly insecure but naive way to implement block cipher with a PRP.

We take our PRP and our message, and break the message up into linear blocks with equal
size as the PRP, then run each block through the cipher.

Problem: if block =m1 = m2=, then cipher output block =c1 = c2=, which means an
attacker gains knowledge about the message.

An adversary has advantage 1.

***** Deterministic Counter Mode
Builds a stream cipher out of the block cipher.

Given a PRF F and a message with L blocks, we evaluate =F(k, i)= for each i in [0, L],
which creates a one-time pad, and then we xor the message with this sequence of PRF outputs
to produce a cipher text.

Thm: For any L > 0. if F is a secure PRF over (K, X ,X), then E_detctr is a
semantically secure cipher over (K, X^L, X^L).

**** Many-time Key
Applicable in filesystems or internet security, where the same key needs to be
used to encrypt a number of files/packets.

***** Semantic security in many-time key context with Chosen Plaintext Attacks
Adversary gets to mount a chosen-plaintext attack (CPA) in which he submits two
arbitrary messages of the same length, completely of his choice, and must
determine if he got the encryption of m1 or m2.

After getting back the first encrypted message, adversary can choose a new
plaintext to query the challenger with and get the next encryption.

In any given experiment, adversary will always get either the encryption of the
left message or of the right message

Deterministic encryption cannot be secure under chosen-plaintext attacks! E.g.,
if an encryption scheme always emits the same cipher text for the same message
the attack has advantage 1.

**** Randomized Encryption
We use some random bits to influence the generator each time, and those bits are
encoded into the cipher to make the decryption invertible. I.e., the cipher maps
message m to one of any N points in a ball of output, and the decryption
algorithm maps every point in that ball back to the message M.

Solves our problem, but increases size of cipher.
CipherText size = Plaintext Size + "# random bits" in our randomized encryption.

Let =F: K x R -> M= be a secure PRF.
Then for a given message m, define =E(k,m) = [r <- R, output (r, F(k,r) xor m)]=

This is semantically secure under CPA provided R is large enough to never
repeat.

**** Nonce-based Encryption
Define =E(k,m,n) = c=, and =D(k,c,n) = m=.

(k, n) pair never repeats, so we get the effect of having new keys each time.
The nonce need not be secret or even random; it just has to be unique!

A simple counter works, provided it doesn't repeat with the same key. HTTPS uses
this, and since the encrypter and decrypter keep the same state, we don't have
to transfer the nonce over the wire.

***** Semantic security
The system must be secure when the nonces are chosen by the adversary; that
said, the adversary must choose distinct nonces during the trial.

**** Cipher Block Chaining with Random Initialization Vector (IV)
Let =(E,D)= by a PRP.  Define =E_cbc(k,m)= that chooses random IV in X and do:

For the 0th cipher block, we emit =c_0 = E(k, m[0] xor IV)=
For the ith cipher block, we emit =c_i = E(k, m[i] xor c_(i-1))=

That is, the first block gets xor'd with the IV before going through E,
and every other block gets xor'd with the previous output cipher block.

There's a theorem that lets us compute how many blocks we can encrypt before we
need to change key.

Note that if the adversary can predict the IV that you will use, he can break
semantic security. TLS 1.1 has this vulnerability and there are attacks based on
it.

**** Cipher Block Chaining with Nonce
We take a nonce and a pair of keys =(k, k1)= and encrypt the nonce with =k1=,
then use the algorithm as before with the encrypted nonce as our IV.

Nonce must be unique: =(key, nonce)= pair is used for only one message

**** Randomized Counter Mode
Choose a random IV for every message, and then use the elements of that IV to
encrypt each block of the message.

For the ith message block m_i, we emit =c_i = E(k, (IV + i) xor m_i)=
Because each cipher block is independent of everything except the initial
randomly chosen IV, we can compute this in parallel.

Return the IV prepended to the output cipher
- Fully parallelizable
- CTR Mode is superior to CBC in every way

* Week 3: Message Integrity
Goal for this week is to provide integrity, without any confidentiality.

This requires a shared secret key, since Mallory can change Alice's message and
CRC simultaneously.

** MACs: Message Authentication Codes
Definition: MAC I = S(V) defined over (K, M, T) is a pair of algorithms:
1. A signing algorithm S(k, m) that outputs t in T
2. A verification algorithm V(k, m, t) outputs yes or no.

such that for every key k in K, and for all m in M, V(k, m, S(k, m)) = "yes"
(consistency requirement).

*** Secure MACs
Attacker gets a to mount a chosen message attack.

For attacker-provided m1, m2, ..., mq attacker is given =t_i <- S(k, m_i)=

Attacker's goal is existential forgery: produce some new valid message/tag pair (m,t)
that is distinct from any (m_i, t_i)

Same semantic security definition as before.

*** MACs implemented with PRFs
Let =F: K x X -> Y= be a secure PRF. Then we can define a MAC =I = (S,V)= as
Signing: S(k,m) = F(k, m)
Verifying: V(k,m,t) = if t = F(k,m) then "yes" else "no"

THM: If =F: K x X -> Y= is a secure PRF and 1/|Y| is negligible, then I is a secure MAC.
Proof: Adversary gets advantage 1/|Y| against a truly random F

Since every secure PRF is a secure MAC, provided its output size is large
enough, we can re-use ciphers like AES for 16-byte messages.

Given this MAC for small messages, there are two common constructions for
expanding this to messages of arbitrary length: CBC-MAC and HMAC.

** Message Integrity Constructions
*** CBC-MAC and NMAC
Used in the Automated Clearing House (ACH).

Encrypted CBC-MAC: run a CBC Chain block cypher with F(k1, .) for each block,
and then a final cypher with F(k2, output) -> tag.

Nested MAC:
We break the message m up into blocks as before, and we start with an intial key k0.
Starting from the first block and first key, we run F(k0, m0), and the output of this
becomes k1 for the F(k1, m1), and so on.

Then we pad t with fpad to get it at length of m block, use a new key, and do a final
F(k, t) -> tag.

Without the last encryption step, this is a "cascade" and vulnerable to extension attacks.

**** Security theorems
For any L > 0, for every efficient q-query PRF adv. A attacking F_ECBC or F_NMAC
there exists an efficient adversary B such that

Adv[A, F_ECBC] <= Adv[B, F] + 2q^2 / |X|
Adv[A, F_NMAC] <= q * L * Adv[B, F] + q^2 / (2 *|K|)

So after MACing ~2^48 messages with AES, we have to change our key in ECBC.

That is, they're insecure after signing X^(1/2) messages with ECBC or K^(1/2)
messages with NMAC.

**** Extension Attacks
For any two distinct messages m1, m2, if they have the same MAC, then m1 || w == m2 || w

That is, if they collide on a tag, then their extensions will also collide.

Birthday paradox attacks start becoming relevant if attacker can make many
queries relative to the key space.

*** MAC Padding
If we just use a naive 0 pad, then the attacker obtains m||0 forgery after querying m.

ISO pad standard: pad with 1000... Then we know to drop all the 0s and the first
1 at the end of the message. If the message is a multiple of the block length
initially, then we add a dummy 100000... block.

**** CMAC
NIST Standard for randomized padding function.
Variant of CBC-MAC where key = (k, k1, k2)

At the last step, if it's not aligned, we add a pad and xor with k1.
If it's alligned, we xor with k2.

The two distinct keys resolve the ambiguity between the "nicely aligned" vs.
"not aligned but padded" cases.

** PMAC and Carter-Wegman MAC
The previous MAC functions weren't parallelizable.

*** Parallel MAC (PMAC)
For the ith block of the message, run a function P(k,i),
xor the results with the message block, then run the results
through F(k, m).  Do this for all messages, and finally run
one more time for a tag (as before).

Since P(k,i) does not depend on previous output and each
F(k, P(k,i) xor m_i) does not depend on other m_i computations,
we can fully parallelize this without subject ourselves to reordering
attacks.

**** Incremental Recomputation
If k blocks change, we can update the tag in O(k) time.

If 1 message block changes, we can recompute the tag by
inverting the Final F(k1, tag), which results
in an xor of many blocks, then we can xor out m_i and xor
in m_i' and re-apply the tag generating function.


*** One-time MAC
In this construction, attacker only gets to send 1 message query.

One-time MACs are generally faster than PRF-based MACs, and just as secure
(though they can only be used once).

Let q be a large prime, slightly larger than the block size.
Let key = (k,a) be two random ints in [1,q]
Let msg = (m1, m2, ..., mL) where each block is a 128 bit int

Then S(key, msg) = P_msg(k) + a (mod q)
where P_msg(x) = m[L] * x^L + ... + m[1]*x is a polynomial of degree L.

*** Carter-Wegman MAC
We can expand fast one-time MACs into many-time MACs with techniques that
use the one-time fast MAC for the O(n) message, then a slower general
MAC

THM: CW((k1,k2), m) = (r, F(k1,r) xor S(k2,m))
is a secure many-time MAC.

** Collision Resistance
Let H: M -> T be a hash function, where |M| >> |T|.

A collision for H is a distinct pair m0, m1 in M such that H(m0) = H(m1).

A function H is collision resistant if for all explicit, efficient algorithms A,
the Adv[A, H] = Pr[A outputs collision for H] is negligible.

Example: sha256

*** MACs from Collision Resitance
Let I = (S,V) be a secure MAC for short messages over (K,M,T).
Let H: M^big -> M.  Then I^big = (S^big, V^big) over (K, M^big, T) as
S^big(k,m) = S(k, H(m)); V^big(k,m,t,) = V(k, H(m), t)
is a secure MAC.

That is, if we have a collision resistant hash function and a secure MAC
for short messages, we have a MAC for big messages by using the hash to
shorten the message first.

E.g., S(k,m) = AES(k, SHA-256(m))

*** Generic birthday attack
Attack on collision resistance.

Let H:M -> {0,1}^n be a hash function, where |M| >> 2^n.
Then there exists a generic alg. to find a collision in time O(2^(n/2)) hashes.

1. Choose 2^(n/2) distinct random messages in M
2. For i = 1,...,2^(n/2) compute t_i = H(m_i)
3. Look for a collision (t_i = t_j). If not found, go back to step 1

Video contains a rigorous proof and discussion of the birthday paradox.

*** Merkle-Damgard Paradigm for Constructing Collision Resistant Hash Functions
Goal: Given a collision resistant hash for short messages, build a collision
resistant hash for long messages.

Given small-size-input collision resistant hash function =h: T x X -> T=,
we obtain =H: X^(<=L) -> T= by:

Break the message M up into blocks m0, m1, ..., mN.
Starting with a fixed IV and m0, compute h(IV, m0).
Then for m_i where i > 0, compute m_i = h(previous hash output, m_i)

That is, we chain the output of the previous hash as the input to the next hash,
along with the message.

We end the message with a =1000000 || 64-bit msg length= number.
This handles non-aligned messages and prevents extension attacks.


THM: If h is collision resistant, then so is the Merkle-Damgard function H

Proof: We will demonstrate that a collision on H implies a collision on h.
Suppose H(M) = H(M').
Let IV = H0, let H1, ..., Ht, H_t+1 = H(M)
Let IV = H0', let H1', ..., Hr', H'_r+1 = H(M')

Because H(M) = H(M'), then H_t+1 = H'_r+1

H_t+1 = h(Ht, Mt || PB)
H'_r+1 = h(Hr', Mr' || PB')

If Ht != Hr', or Mt != Mr', or PB != PB', then we have a collision, and we're done.

If all 3 of these are the same, then the lengths are the same (t = r) since the
PB contains the length of the message, and we can continue down to the previous
block.

Ht = h(H_t-1, M_t-1) = H't = h(H'_t-1, M'_t-1)
If H_t-1 != H'_t-1 or M_t-1 != M'_t-1, then we're done, because we have a compression
function collision on h.

Otherwise, we continue. Eventually, we will either find a difference, or we will
get to the first block and will have proven that the entire messages are the
same, which is also a contradiction.

*** Building secure compression functions
This section will construct the =h: T x X -> T= to give to the Merkle-Damgard
construction H.

We can build compression functions from block ciphers.

THM: Let =E: K x {0,1}^n -> {0,1}^n= be an ideal block cipher (collection of |K|
random permutations). Then for the Davies-Meyer compression function =h(H, m) =
E(m, H) xor H=, it takes O(2^(n/2)) evaluations of (E,D) to find a collision
=h(H,m) = h(H', m')=.

Because the birthday attack demonstrates O(2^(n/2)) is the theoretical best-case
performance for any hash, this means Davies-Meyer is optimal.

**** SHA-256
Merkle-Damgard function with a Davies-Meyer compression function using the
SHACAL-2 block cipher.

This involves a 256-block + 512-bit key => SHACAL-2 => 256-bit block.

**** Provable compression functions
Compressions built on number theory.

Choose a random 2000-bit prime p and random 1 <= u, v <= p.
For m,h in [0, p-1] define h(H,m) = u^H * v^m (mod p)

Then finding a collision for h(.,.) is as hard as solving "discrete-log" modulo p,
which is NP.

** Hash-MAC (HMAC)
A MAC from SHA-256 with a 256 bit output.

Given an inner-pad ipad and outer-pad opad, and a key k, we run:
HMAC: S(k, m) = H(k xor opad, H(k xor ipad || m))

ipad and opad are fixed 512-bit constants specified in the HMAC standard.

Similar to the NMAC PRF, but the two keys are dependent.

** Timing attacks on MAC Verification
Specifically, looking at the Keyczar python crypto library HMAC implementation.

#+BEGIN_SRC python
def verify(key, msg, sig_bytes):
    return HMAC(key, msg) == sig_bytes
#+END_SRC

Problem: the comparator == is implemented by looping on the bytes, and exiting
as soon as one of them is found to be different.

Timing attack:
1. Query server with random tag
2. Loop over all possible first bytes and query server. Stop when verification
   takes a little longer than in step 1.
3. Repeat for all tag bytes until valid tag found

Defense 1: make the comparison always take the same amount of time
#+BEGIN_SRC python
def verify(key, msg, sig_bytes):
    # return false if sig_bytes has wrong length
    result = 0
    for x, y in zip(HMAC(key, msg), sig_bytes):
        result |= ord(x) ^ ord(y)
    return result == 0
#+END_SRC

Must ensure compiler optimizaiton doesn't mess you up!

Another clever defense: prevent the attacker from knowing the values being compared
#+BEGIN_SRC python
def verify(key, msg, sig_bytes):
    mac = HMAC(key, msg)
    return HMAC(key, mac) == HMAC(key, sig_bytes)
#+END_SRC

Here, what we're doing is first computing the HMAC, then running that value
through another HMAC, an drunning sig_bytes through HMAC, and comparing THOSE
values. Byte-by-byte comparator will output false on first diff, but the adversary
doesn't know the vlaues.

Wasteful, but no danger of compiler optimization.
* Week 4: Authenticated Encryption
So far, we've shown confidentiality, but not integrity, against chosen plaintext
attacks.

We've also show integrity without confidentiality (MAC)

This week we will combine things to get confident and integrity-proof traffic against
adversaries who can modify/block/inject packets on the network.


** Activate attacks on CPA-Secure Encryption
In a secure TCP/IP stack, a sender encrypts a package with data and a destination IP port,
the TCP/IP stack on the target computer receives the packet, decrypts it, and forwards it
to the application listening on that local port.

If data is encrypted with CBC with random IV, an attacker can change the destination port
trivially and wait for the TCP/IP stack to decrypt it and send the plaintext to his port.

I.e., the attacker knows the offset, and can send (80 xor 25) to change the port from 80 to 25.

There are also "active attacks" where the attacker doesn't need to be on the
host; instead he listens to a packet, then keeps modifying it and
re-transimtting it to the server, building up tables based on whether the server
sends an ack on the packet reception with the checksum.

** Chosen ciphertext attacks
Adversary has ciphertext c that it wants to decrypt; adv. can fool server into
decrypting certain ciphertexts (but not c directly).

*** Semantic Security in the context of chosen ciphertext security
Adversary can mount both a chosen plaintext attack and a chosen ciphertext attack.
I.e., he can obtain the encryption of arbitrary messages of his choice, AND he
can decrypt any ciphertext of his choice other than the challenge ciphertext.

His goal is to break semantic security under these conditions.

More formally,
Let E = (E,D) be a cipher defined over (K,M,C).  For b = 0,1, define EXP(b):

for i in [1..q], the adversary submits either:
  1. CPA query: gives (m_i_0, m_i_1), gets back c_i <- E(k, m_i_b)
  2. CCA query: gives c_i for c_i not in { c1, ..., c_i-1 }, gets back m_i <- D(k, c_i)

Adversary must guess b.

Note the clause on item 2: he can't submit a cipher text given as the result of a CPA query, since
otherwise he'd find out immediately if the cipher was m_0 or m_1.

**** CBC with rand. IV is not CCA Secure
Adversary submits distinct (m0, m1) with |m_i| = 1, and gets back c = E(k, m_b) = (IV, c[0]).
Then he submits the CCA query c' = (IV xor 1, c[0]), and will receive back m_b xor 1, and can tell
with advantage 1 what the value of b is.

*** THM: Authenticated Encryption implies CCA security
Let (E,D) be a cipher that provides A.E.  Then (E,D) is CCA secure.


** Constructions from ciphers and MACs
Many projects have incorrectly combined CBC and MACs.

Examples: given message m, encryption key k_e, and MAC key k_i,
there are a couple ways we can go:

*** Mac then encrypt
SSL: m => (m || S(k_i, m)) => E(K_e, m || tag)
That is, we compute the tag, concat it to the message, and output the encryption of that.

*** Encrypt then mac
IPsec: m => E(k_e, m) => c || S(k_i, c)
We encrypt the message, and then output the cipher and tag of the cipher.

*** Encrypt and mac
SSH: m => c = E(k_e, m) => output c || s(k_i, m)
This is not A.E., because many tags might include a few bits of the
plain text

*** Standards
Of these, the IPsec method will *always* provide Authenticated Encryption.
SSL is mostly secure, but there are vulnerable pathalogical examples.

GCM: CTR mode encryption then CW-MAC
CCM: CBC-MAC then CTR mode encrption (802.11i)
EAX: CTR mode encryption then CMAC

GCM has Intel instruction support.

All of these are nonce-based and AEAD: Authenticated Encryption with Associated
Data. This supports things like network packets, where we have an unencrypted
but authenticated header for routers and an encrypted payload for clients.

OpenSSL API:
#+BEGIN_SRC c
int AES_GCM_Init(AES_GCM_CTX* ain,
                 unsigned char* nonce,
                 unsigned long noncelen,
                 unsigned char* key,
                 unsigned int klen)
#+END_SRC

*** OCB
In most AE constructions, for every 1 block of plaintext we have to evaluated 2
cipher encryptions: one for the encryption, and once for the MAC.

OCB is a construction that lets us just go through once to produce both the
encryption and the MAC.

It's generally superior from a performance standpoint, but not used due to
various patents.

** Case study: TLS 1.2
Authenticated Encryption in the rea world.
Uses the TLS Record Protocol: every TLS record (<16KB, encrypted data) starts with a Header.

Unidirectional keys: k_(browser -> server) and k_(server -> browser). Both sides
know both keys, generated by TLS Key Exchange Protocol.

Stateful encryption:
- Each side maintains two 64-bit counters: ctr_b->s, ctr_s->b
- Init to 0 when session started
- Provides replay defense

Encrypting the TLS Record: encryption with CBC-AES-128, MACing with HMAC-SHA1.
k_b->s = (k_mac, k_enc).  This means there are 4 overall keys (server has mac and enc keys as well)
Browser side: enc(k_b->s, data, ctr_b->s):
1. tag <- S(k_mac, [++ctr_b->s || header || data])
2. Pad [ header || data || tag] to AES block size
3. CBC encrypt with k_enc and new random IV
4. Prepend header, which includes type || version || length unencrypted

TLS record: decryption of (k_b->s, record, ctr_b->s):
1. CBC decrypt record using k_enc
2. Check pad format: send bad_record_mac if invalid
3. Check tag on [++ctr_b->s || header || data ], send bad_record_mac if invalid

Provides authenticated encryption! Counters prevent replay attacks and don't
need to be sent in the record, so they cost no bandwidth. Note that it's important
that the server does NOT distinguish between bad pad vs. bad MAC.

** CBC Padding Attacks against incorrect implementations
Padding oracle attack: when the attacker can tell the difference between a pad error vs. a MAC error.
(This worked against TLS 1.1).

There are also padidng oracle timing attacks: if the server fails decryption it
may return quickly, and if it decrypts successfully but fails MACing, it may
take longer.

*** Using a padding oracle
Attacker has ciphertext c = (c[0], c[1], c[2]) and wants to get m[1]

1. Let g be a guess for the last byte of m[1].
2. Attacker will recompute c[0]' = c[0] xor g xor 0x01
   When c[1] is decrypted, the last byte will be xor'd by g xor 0x01
   If the last byte == g, then the pad is well-formed (just the number 1, for 1 byte).
   If the guess is not correct, we'll get an invalid pad.
3. Do this on average 128 times to get the last byte g.
4. Use a (02, 02) pad to learn the next byte and so on.

In TLS, if the server receives a bad mac or bad pad, it will restart and
regenerate all the keys.

However, in IMAP over TLS( protocol for reading email), every 5 minutes client
sends a login message to server. This means every 5 minutes the attacker gets an
encryption of password with a new key, and can make 1 guess against the server's
padding oracle (which will likely cause it to reset). User's PW can be recovered
in a few hours using this technique.

** Attacking non-atomic decryption
Attack on SSH Binary Packet Protocol, which uses encrypt-and-mac with CBC encryption and chained IV.

Every ssh packet contains:
(sequence number || packet len || pad len || payload || pad, mac tag over plaintext version of L.H.S.)

Everything except the sequence number and mac tag is encrypted. Mac is computed
over plaintext!

Decryption works as follows:
1. Decrypt packet length field only (!)
2. Read from network as many packets as length specifies
3. Decrypt remaining ciphertext blocks
4. Check MAC tag and send error response if invalid

Let's say the attacker has one cipher block c = AES(k,m) and wants m.
Attacker feeds bytes one-by-one to server.  The server will decrypt the first
block to read the length, and continue accepting bytes until it's accepted
len bytes, at which point it will return a MAC error.

Attacker has just learned first 32 bits of m. Here, non-atomic decryption means
the server decrypts and uses the length fied before it is authenticated.

** Key Derivation
How to derive many keys from one key.

Let F be a secure PRF with key space K and outputs in {0,1}^n.  Supose source key SK is uniform in K.
Define the Key Derivation Function (KDF) as:
KDF(SK CTX, L) := F(SK, (CTX || 0)) || F(SK, (CTX || 1)) || ... ||| F(SK, (CTX II L))

where CTX (context) is a string that uniquely identifies the application. We use
this to generate as many bits as we want, and then cut off the length whenever
we have enough keys.

The CTX separates applications that may be using the same source key (which
might come from a hardware RNG or key exchange).

*** Extract-then-expand paradigm
Extract a pseudo-random key k from the source key SK, used when the SK is not
itself uniformly randomly distributed (e.g., it has some bias in the key space).

An extractor takes a bumpy distribution and makes it a uniform distribution in
the key space (or indistinguishable from uniform).

Extractors take a salt: a fixed non-secret string chosen at random.

*** HKDF: a KDF from HMAC.
Implements the extract-then-expand paradigm

Etract: use k <- HMAC(salt, SK)
Public salt value is used as the HMAC key.

Expand using HMAC as a PRF with session key k.

*** PBKDF: Password-based KDF
We can derive session keys from passwords. Passwords rarely have sufficient entropy for HKDF,
since derived keys will likely be vulnerable to dictionary attacks.

PBKDF defenses: use a salt and a "slow hash function".

Standard approach: PKCS#5 (PBKDF1) is the standard: H^c(pwd || salt): iterate hash function c times.

** Deterministic Encryption
No nonces.  Given a message m, always map it to c.
Used for lookups in encrypted databases.  This lets us encrypt the index with k1, the data with k2,
and later do O(log n) db queries to lookup data by key by looking up encrypt index with k1.

Leads to significant attacks if the message space M is small, since an attacker
can build a dictionary of cipher texts and learn what the decryptions of those
cipher texts are.

*** Unique messages
Suppose the encryptor never encrypts the same message twice: the pair (k,m) never repeats.
Then we don't have to worry about the above attack.

*** Deterministic CPA Security
We play the standard chosen plaintext attack game, except during the attacker's q queries
the left messages must always be distinct amongst themselves, and the right messages must
always be distinct among themselves.

Then the attacker will never see two messages encrypted under the same key.

*** CBC with Fixed IV is not Detereministic CPA Secure
Let E: K x {0,1}^n -> {0,1}^n be a secure PRP used in CBC.

An adversary submits (m0, m1) = (0^n 1^n, 0^n 1^n) and gets back
c1 = [ FIV, E(k, 0^n xor FIV), ... ]

Then the adversary submits (0^n, 1^n) and receives either:
c <- [ FIV, E(k, FIV) ] or
c <- [ FIV, E(k, 1^n xor FIV) ]

Leads to significant attacks in practice.

Counter mode with a fixed IV is also not deterministic CPA secure.

*** Synthetic IV (SIV) and wide PRP
https://www.coursera.org/learn/crypto/lecture/hM7f2/deterministic-encryption-siv-and-wide-prp
This is what to do when you want deterministic encryption

Let (E,D) be a CPA-Secure encryption: E(k, m; r) -> c
Let F: K x M -> R be a secure PRF

Define E_det((k1,k2), m) => r <- F(k1, m)
                            c <- E(k2, m; r)
                            output c

Then E_det is semantically secure under deterministic chosen plaintext block.

**** SIV ensures ciphertext integrity for free
We want to build Deterministic Authenticated Encryption (DAE).

Because the IV is generated from a Function that takes both the key and the
message, the output of F is dependent on the message value. Once we decrypt the
ciphertext with the IV, we can use the decrypted message + F + k1 to see if we
would generate the same IV.

**** Just using a PRP for DAE
Let (E,D) be a secure PRP, E: K x X -> X

THM: (E,D) is semantically secure under CPA.

As a result, we can directly use AES for (non-integrity-based) CPA secure
encryption for <= 16 byte messages.

**** Wide block PRP
If we have a larger msg space, we construct a wide block PRP.

Let (E,D) be a secure PRP E: K x {0,1}^n -> {0,1}^n.
Let EME be a PRP on {0,1}^N for N >> N

How it works: key = (K, L)
m <- mp xor mc

We break our message x and break it into blocks, then xor each ith block with a padding function
P(L, i).

We xor all the Ps together, to form mp, then we encrypt to get mc.

Then a lot more stuff happens. Come back to this if I need to.

Takeaway: this is about 2x slower than SIV, but requires a constant factor less
data, so it's good for very small messages.

** Tweakable Encryption
Disk encryption: sectors on disk are fixed size (e.g. 4KB), so we want encryption
where we cannot expand the plaintext (i.e., M = C).

Must use deterministic encryption, so there's no randomness since there's no
room to store the randomness, and we can't have integrity, since there's nowhere
to store a tag.

Idea: use a different key for each sector, to avoid the leakage problem where
identical plaintext sectors have the same ciphertext.

Generate all the keys with k_t = PRF(k, t), where k is some master key, and then use key
t to encrypt block t.

*** Tweakable Block Ciphers
Goal: construct many PRPs from a key k in K.

E,D: K x T x X -> X
where T is a tweak input from the tweak space.

for every t in T and k in K, E(k,t,.) is an invertible function on X,
indistinguishable from random.

Application: use the sector number as the tweak.

**** Trivial tweakable construction
E_tweak(k, t, x) = E(E(k, t), x)
i.e., we encrypt the tweak using our key, then encrypt the data using our resulting random key.

Requires 2n evaluation of E(.,.)

**** XTS Tweakable block cipher
Let (E,D) be a secure PRP, E: K x {0,1}^n -> {0,1}^n
XTS: E_tweak((k1, k2), (t, i), x) = N <- E(k2, t)
                                    m xor P(n, i) where P is some lightweight padding function
                                      => E(k1, .) xor P(N, i) -> c

** Format Preserving Encryption (FPE)
Comes up with encryption of credit card numbers.

End-to-end encryption: we'd like to have the point-of-sale terminal encrypt the
CC in a way that it looks like a valid credit card, then pass it through many
middle-men, and have the receiving bank decrypt it.

Given 0 < s <= 2^n, build a PRP on {0,...,s-1} from a secure PRF F: K x {0,1}^n -> {0,1}^n

Steps:
1. Shrink PRF from {0,1}^n to {0,1}^t, t < n, such that 2^(t-1) < s <= 2^t
2. Luby-Rackoff with F': K x {0,1}^(t/2) -> {0,1}^(t/2)
3. Given PRP (E,D): K x {0,1}^t -> {0,1}^t, we build (E',D'): K x {0,...s-1} -> {0,...,s-1}
by doing on E'(k,x) for input x in {0,...,s-1} do:
y <- x; do { y <- E(k,y) } until y in {0,...,s-1}; then output y

That is, we have an encryption function, and we keep applying it until we get a
value in our target set.

Since 2^(t-1) < s <= 2^t, we will on average need 2 iterations for this process to converge.

* Week 5: Basic Key Exchange
Public key cryptography.

** Key management with trusted 3rd parties
Problem: n users, storing all mutual secret keys is difficult.
Each user has to store and manage n keys to talk to n users.

Online trusted 3rd party (TTP): each user shares 1 key with the TTP.

*** Toy protocol secure against eavesdropping (but not active attacks)
When Alice wants to talk to Bob, she will ask the TTP for a key_ab.
It will generate one, encrypt it with Alice's key, and send it back to her.
E(k_a, "A,B" || k_ab)

The TTP sends a ticket with contents: E(k_b, "A,B" || k_ab)
Bob can decrypt this key, which will contain the key ab and a secure
text msg from the TTP saying that the key is for Alice,Bob communication.

Alice can send the ticket to Bob, and he can decrypt it with his key, and then
they have a shared ticket.

Active attackers can destroy this with replay attacks. If Alice has a secure
session with Bob, an attacker can record and replay a session between Alice and
Bob, which could have harmful effects if the session wasn't idempotent!

This is the basic idea of Kerberos, which is also secure against active attacks.

** Basic key exchange without a TTP: Merkle Puzzles
For now, still only secure against eavesdropping.

This does a (very inefficient) key exchange with only the block cipher tools
we've seen so far.

Given E(k,m) a symmetric cipher with k in {0,1}^128.
Let puzzle(P) = E(P, "message") where P = 0^96 || b1..b32
Goal: find P by trying all 2^32 possibilities.

Alice generates 2^32 puzzles, generated by:
for i in 1, ..., 2^32, choose random P_i in {0,1}^32 and xi, ki in {0,1}^128.
  set puzzle_i <- E(0^96 || P_i, "Puzzle #x_i" || k_i)
Send puzzle_1, ..., puzzle_2^32 to Bob

Bob chooses a random puzzle_j and solve it.  Obtains (x_j, k_j) in time O(2^32).
Sends x_j to Alice
Alice looks up puzzle x_j in her DB and knows we're using key k_j.

Alice and Bob each do O(n) work, but the eavesdropper has to solve all the
puzzles, which takes O(n^2) time.

Not used in practice, since each participant has to spend significant work to
set this up.

Takeaway: there's a quadradic gap in work done by the participants and the
attacker. Unproven, but it's believed that quadradic is the best we can do with
a block cipher.

** The Diffie-Hellman Protocol (1976)
Protocol:
Fix a large prime p (e.g., 600 digits, or ~2000 bits).
Fix an integer g in {1,...,p}
Alice chooses a random a in {1,...,p-1}.
Bob chooses a random b in {1,...,p-1}.

Alice computes A = g^a mod p and transmits this in the clear to Bob
Bob computes B = g^b mod p and transmits this in the clear to Alice

Then k_ab = g^(ab) mod p

Alice can compute B^a mod p = (g^b)^a mod p = g^(ab) mod p
Bob can do the same with A^b to get k_ab = g^(ab) mod p

Eavesdropper sees p, g, A, B, but can't compute g^(ab) mod p

Computing this is reducible to the discrete log function, which is a
sub-exponential problem, roughly e ^ (n / 3)

*** Problem: the cube root in the exponential means we need huge primes
In order to double our security, we need to increase the prime length by a factor of 8.

Instead of using modulo arithmetic, we can use elliptic curves to get
significantly better security.

*** Insecure against MITM, as stated here
We'll come back to this next week.

Alice sends A = g^a -> MitM, who sends A' = g^a' to Bob.
Bob sends B = g^b -> MitM, who sends B' = g^b' to Alice.

Alice will end up with key g^(ab')
Bob will end up with key g^(a'b)
MitM knows both values, and can compute both keys, decrypt Alice's messages,
re-encrypt them with Bob's key, and send them to Bob, and vice versa.

*** DH is non-interactive
Users can post their g^n to a public location, and the users can go to the
public location and establish a secure key for communicating with each other
without doing any interactive key-exchange!

** Public-key Encryption
A public-key encryption system is a triple of algs (G, E, D0), where
- G(): randomized algorith that outputs a key pair (pk, sk)
- E(pk, m) randomized alg that takes m in M and outputs c in C
- D(sk, c): deterministic algorithm that takes c in C and outputs m or bottom

such that for all (pk, sk) output by G, and for all m in M, D(sk, E(pk, m)) == m
(consistency property)

*** Semantic Security in public-key encryption
For b = 0,1, define experiments EXP(0) and EXP(1) as:

(pk, sk) <- G()
Adversary receives pk.
Adversary submits (m_0, m_1) in M s.t. |m_0| == |m_1|.
Receives c <- E(pk, m_b), adversary must determine b with non-trivial advantage.

Note that chosen plaintext attacks make no sense, since with the pk the attacker can
already get a cipher for anything he wants.

*** Interactive protocol for public-key key exchange
Alice generates (pk, sk) <- G() and sends "Alice, pk" to Bob
Bob chooses random x in {0,1}^128, sends back "Bob, c <- E(pk, x)"
Alice decrypts c with her sk and now they use x as a key.

Semantically secure against eavesdropping.
Note: still vulnerable to MitM

** Intro to Number Theory: Notation
We wil use number theory to build key exchange protocols, digital signatures,
and public-key encryption.

*** Notations
N denotes a positive integer
p denotes a prime
Z_n denotes {0,1,...,n-1}

Z_n denotes a ring with addition and multiplication modulo N

*** Modular arithmetic examples
Let N = 12
9 + 8 = 5 Z_12
5 x 7 = 11 Z_12
5-7 = 10 Z_12

*** Greatest Common Divisor (GCD)
For ints x, y, let gcd(x,y) be the greatest common divisor of x,y,
where greatest common divisor is the largest integer that evenly divides
both x and y.

For all integers x,y, there exist ints a,b such that
a*x + b*y = gcd(x,y)
and a,b can be found efficient using the Extended Euclid Algorithm
in time O(log^2(n))

x,y are relatively prime if gcd(x,y) = 1

*** Modular inversion
The inverse of x in Z_n is an element y in Z_n such that xy = 1 in Z_n

Lemma: x in Z_n has an inverse iff gcd(x,N) = 1

Proof: Let gcd(x,N) = 1.  Then there exists a,b such that a*x + b*N = 1
Reducing by module N, then a*x + 0 = 1 in Z_n
Then x inverse is simply a.

Other direction:
if gcd(x,N) > 1, then for all a: gcd(a*x, N) > 1 then a*x D.N.E. 1 in Z_n

*** Invertible elements
Let Z_n* = { set of invertible elements in Z_n } = { x in Z_n : gcd(x,N) = 1 }

Example: for prime p, Z_p* = Z_p \ { 0 }, namely everything in Z_p is invertible except 0.

** Fermat and Euler
*** Fermat's theorem
Let p be a prime.  Then for all x in Z_p*, z^(p-1) = 1 in Z_p.

For example, for any x in Z_p*, x^(p-1) = 1 => x * x^(p-2) = 1, so
the inverse of x is x^(p-2)

The runtime for this is O(log^3(p))

**** Application: generating random primes
If we want to generate a large random prime, p ~= 2^1024, then we can
1. choose a random integer p in [2^1024, 2^1025-1].
2. Test if 2^(p-1) = 1 in Z_p.  If so, output p and stop. Otherwise, goto step 1.

Probability that it passes this test and is not a prime is extremely small, and
gets vanishingly small as the size of p increases.

*** The structure of Z_p*
THM: (Euler): Z_p* is a cyclic group, that is there exists a generator of Z_p* g
in Z_p* such that {1, g, g^2, ..., g^p-2} = Z_p*.

Example: For p = 7, 3 is a generator, since
{1, 3, 3^2, 3^3, 3^4, 3^5 } = {1,3,2,6,4,5}


For g in Z_p*, the set {1,g,g^2,...} is called the group generated by g, denoted <g>
This is called a multiplicative group.

Def: the order of g in Z_p* is the size of <g>, denoted |<g>|

In the above example, |<g>| = 6

The order of the group generated by 2 in Z_7 is 3
and ord_7(1) = 1

*** Lagrange's Theorem
THM: (Lagrange) For all g in Z_p*, ord_p(g) divides p-1

*** Euler's generalization of Fermat
For an integer N, define phi(N) = |Z_n*|
Example: phi(12) = | {1,5,7,11} | = 4
phi(p) = p - 1, since the set doesn't contain 0

If N = p*q for prime p and q, then phi(N) = N - p - q + 1 = (p-1) * (q-1)

THM: For all x in Z_n*, x ^ ( phi(N) ) = 1 in Z_n

Example: 5 ^ (phi(12)) = 5^4 = 625 = 1 in Z_12

** Modular e'th roots
https://www.coursera.org/learn/crypto/lecture/fjRVO/modular-e-th-roots
We know how to solve modular linear equations
a*x + b = 0 in Z_n.

*** Definition of e'th root
Let p be a prime and c in Z_p.
Def: x in Z_p such that x^e = c in Z_p is called an e'th root of c.

Example: The cube root of 7 in Z_11 is 6, since 6^3 = 18 = 7 Z_11.

*** When does an e'th root exist

**** Easy case: e coprime with p-1
If gcd(e, p-1) = 1, then for al c in Z_p*, c^(1/e) exists in Z_p and is easy to find.

Proof: Let d = e^(-1) in Z_(p-1), which exists because d is coprime with p-1.
Then c^(1/e) = c^d in Z_p.

**** Case e=2
If p is an odd prime, then gcd(2, p-1) D.N.E. 1

Def: x in Z_p is a quadratic residue (Q.R.) if it has a square root in Z_p.
If p is an odd prime, then the number of Q.R. in Z_p is (p-1)/2 + 1

The only elements that have a square root are the quadratic residue elements!

**** Euler's Theorem
THM: x in Z_p* is a Q.R. iff x^((p-1)/2) = 1 in Z_p

**** Computing square roots mod p
Suppose p = 3 (mod 4)

Lemme: if c in Z_p is a Q.R. then c^(1/2) = c^((p+1)/4) in Z_p

*** Solving quadratic equations mod p
Give ax^2 + bx + c = 0 in Z_p,
we can use the quadratic equation:

x = (-b +/- (b^2 - 4ac)^(1/2)) / (2a) in Z_p

We find (2a)^-1 in Z_p using extended Euclid

We find the square root of (b^2 - 4ac) in Z_p if one exists
using a square root algorithm


*** Computing e'th roots mod N
Let N be a composite number and e > 1
When does c^(1/e) in Z_n exist?

Answering these questions requires the factorization of N, as far as we know

** Arithmetic Algorithms
*** Representing bignums
Representing an n-bit integer (e.g. n=2048) on a 64-bit machine is done by
breaking it into n/32 blocks of 32 bits each, so that we can multiple two blocks
of size 32 bits together and still represent it in a 64-bit processor.

Given two n-bit integers, addition and subtraction are linear time

Multiplication naively takes O(n^2). Karatsuba (1960) achived O(n^1.585) Even
better algorithms can do O(n*log(n)), but with very very large constant factors
that make it impractical in real world use.

Division with remainder takes O(n^2)

*** Exponentiation operations
Given a finite cyclic group G (for example G = Z_p*), and given a generator g in
G and x compute g^x.

Repeated squaring algorithm.  Suppose x = 53 = 32 + 16 + 4 + 1 = (110101)_2

Then g^53 = g^32 * g^16 * g^4 * g^1

If we compute g^1 -> g^2 -> g^4 -> g^8 -> g^16 -> g^32 with repeated squaring,
then we can use these numbers and get g^53 above with just a few multiplications.

Implementation: input g in G and x > 0.
Write x = (x_n, x_(n-1), ..., x_0)_2

y <- g, z <- 1
for i = 0 to n do:
  if (x[i] == 1):  z <- z*y
  y <- y^2
output z

Resulting runtime is O(n^3)

** Intractable Problems

*** Modulo primes: The Discrete Log problem
Fix a large prime p>2 and g in Z_p* of order q.

Consider the function: x -> g^x in Z_p
This is easy to compute with repeated squaring.
However, given g^x, finding x (the inverse) is the
discrete log problem:

Dlog_g(g^x) = x where x in {0, ..., q-2}

**** More general DLOG
Let G be a finite cyclic group and g a generator of G.
G = { 1, g, g^2, ... , g^(q-1)}

Def: We say the DLOG is hard in G if for all efficient alg. A:

Given random g <- G, x <- Z_q, Pr[A(G,q,g,g^x) = x] < negligible

Z_p* for large p is a group where DLOG is hard.
Another group is the elliptic curve group mod p.

**** Collision resistance application
Choose a group G where DLOG is hard (e.g., Z_p* for large p)

Let q = |G| be a prime. Choose generators g,h of G.

For x,y in {1,...,q} define H(x,y) = g^x * h^y in G

Lemma: finding collision for H(.,.) is as hard as computing Dlog_g(h).

*** Hard problems with modular composites
Consider the set of integers Z_2(n) := { N = p*q where p,q are n-bit primes }

**** Factoring problem
Factor a random N in Z_2(n)

Best known algorithm (NFS) has run-time exp(n^(1/3)) for n-bit integer

**** Polynomial roots
Given a polynomial f(x) where degree(f) > 1 and a random N in Z_2(n),
find x in Z_N s.t. f(x) = 0 in Z_n.

* Week 6: Public Key Encryption

** Definitions and Security
Notably, public-key encryption can be non-interactive if Alice can publish her
public key somewhere.

It can also be used for session setup, i.e. web browsing.

DEF: A public-key encryption system is a triple of algs (G, E, D)
- G(): randomized alg. outputs a key pair (pk, sk)
- E(pk, m): randomized alg that takes m in M and outputs c in C
- D(sk, c): deterministic alg that takes c in C and outputs m in M or bottom

such that for all (pk,sk) output by G, and for all m in M, D(sk, E(pk,m)) == m
(consistency property)

*** Security against eavesdropping
Standard experiment game with the adversary; adversary gets to send 1 message
and get 1 cipher text back.

Note that in public-key encryption, one-time security implies many-time security (CPA).
This follows from the fact that the attacker can encrypt whatever he wants by himself.

*** Chosen Ciphertext Security
Enc = (G,E,D) public-key encryption over (M, C).  For b = 0,1 define EXP(b):

Challenger gens (pk,sk) with G()
Gives pk to adversary

Chosen Ciphertext Attack phase 1: attacker can submit as many ciphertexts
as he wants and get as many decrypted messages back.

challenge: attacker submits m_0, m_1 and receives a cipher c_i

CCA Phase 2: Attacker gets to submit as many ciphertexts as he wants again,
provided they are not the cipher from the challenge phase.

This implies Indistinguishability under a Chosen Ciphertext Attack (IND-CCA) or CCA Security.

** Public key encryption from Trapdoor permutations
Trapdoor function (TDF):
DEF: A trapdoor func X -> Y is a triple of efficient algs (G, F, F^-1)
- G() gen for (pk,sk)
- F(pk, .) deterministic alg that defines X -> Y
- F^-1(sk, .) defines a function Y -> X that inverts F(pk, .)

Namely, pk goes X -> Y, and sk inverts Y -> X

(G, F, F^-1) is secure if F(pk, .) is a "one-way function", meaning it can be
evaluated but cannot be inverted without sk.

*** Building pub-key system with (G,F,F^-1)
If we have a secure TDF X -> Y and a symmetric auth. encryption (E_s, D_s) defined over (K,M,C),
and a H: X -> K hash function, we consruct a pub-key enc system (G,E,D):

G is the same G for TDF

E(pk, m) is:
  x <- X randomly,
  y <- F(pk, x),
  k <- H(x),
  c <- E_s(k,m)
  output (y,c)

D(sk, (y,c)) is:
  x <- F^-1(sk, y),
  k <- H(x),
  m <- D_s(k, c)

Essentially, the TDF is used to encrypt a random x, and then we use a normal
encryption system.

Security: If (G,F,F^-1) is a secure TDF, (E_s,D_s) provides auth. enc.,
and H: X -> K is a "random oracle", then (G,E,D) is CCA secure

In practice we'd use something like sha256 for H.

** The RSA Trapdoor Permutation
We use a trapdoor function, but have F(pk, .): X -> X as a permutation

*** Review
Let N = p*q where p, q are prime

Z_n = {0,1,2,...,N-1}
Z_n* = { invertible elements in Z_n }

Facts:
- x in Z_n is invertible iff gcd(x,N) = 1
- Number of elements in Z_n* is phi(N) = (p-1)(q-1) = N - p - q + 1
- phi(N) is close to N - 2*(n^(1/2)) which is close to N, so almost all
- Thus almost all elements in Z_n are invertible

Euler's Thm: For all x in Z_n*, x ^ (phi(N)) = 1

*** History
First published in 1977 in Scientific American

Widely used in SSL/TLS certs, key exchange, email and filesystems, etc.

Named for Rivest, Shamir, and Adleman

*** Algorithm
G(): choose random primes p,q on the order of 1024 bits. Set N = p*q
Choose integers e,d s.t. e*d = 1 (mod phi(N))
Output pk = (N, e), sk = (N, d)

F(pk, x): Z_n* -> Z_n*; RSA(x) = x^e in Z_n

F^-1(sk, y) = y^d; y^d = RSA(x)^d = x ^ (ed) = x ^ (k * phi(N) + 1) = (x ^ (phi(N)) ^ k) * x
using Euler's Theorem

*** The RSA Assumption: RSA is a one-way permutation
For all efficient alg.s A:
Pr[A(N,e,y) = y^(1/e) ] < negligible
where p,q <- n-bit primes, N <- pq, y <- Z_n*

*** Real-world pub-key encryption
Plugging RSA into the ISO std, we have a real pub key encryption system!

(E_s, D_s): symmetric encryption scheme providing
H: Z_n -> K whee K is key space of (E_s, D_s)

G(): generate RSA params pk = (N,e), sk = (N,d)
E(pk, m): 1. Choose random x in Z_n
          2. y <- RSA(x) = x^e, k <- H(x), with H as sha256
          3. output (y, E_s(k,m))
D(sk, (y,c)): output D_s(H(RSA^-1(y)), c)


*** Textbook RSA is insecure
If we don't run through the has function, but instead directly use RSA to
encrypt and decrypt with e and d, this is insecure!

For starters, it's deterministic, but on top of that RSA is *only* a trapdoor
permutation, not a full encryption system without the ISO standard.

**** Sample attack with a website
Browser -> Hello to Server
Server -> (e,N) -> Browser, with server storing d locally
Browser generates random session-key k, sends c = RSA(k) -> Server

Suppose k is 64 bits.  Eve sees c = k^e in Z_n

If k = k1 * k2, where k1, k2 < 2^34, then c / (k1 ^ e) = k2 ^ e in Z_n
and now we can do a meet-in-the-middle attack.

1. Build a table c/(1^e), c/(2^e), ... in time 2^34
2. For k2 = 0, ..., 2^34, test if k2^e is in the table, in time 2^34

Now we've found (k1,k2), can multiple them to get k, and completely break the
encryption system.

Total attack time takes much less time than the key size.

** PKCS 1
In the real world, we don't often use the ISO standard

In practice, we often give the RSA system a key rather than having it generate
its own.

So we have msg key -> preprocessing that expands a 128-bit AES key into the 2048
bit key for RSA -> RSA algorithm -> ciphertext

*** Attack on PKCS1 v1.5
Attacker intercepts c
We can send c to to server, and if the 16 MSB != '02', then the server will
return a decryption error.  It turns out this is enough to fully decrypt the msg.

Bleichenbacher (1998)

*** Defense (RFC 5246)
Minimum defense: if the plaintext doesn't begin with 02, we pretend the plaintext
is some random value and continue on, to eventually fail since we'll have different
keys between client and server.

*** PKCS1 v2.0: Optimal Asymmetric Encryption Padding (OAEP)
Better way of doing RSA encryption that has chosen plaintext security

THM: If RSA is a trap-door permutation, then RSA-OAEP is CCA secure when H,G are random oracles.

** Is RSA a one-way function?
To invert RSA without d, attacker must compute
x from c = x^e mod N

Asking how hard is computing e'th roots modulo N?

Best known algorithm:
1. Factor N (hard)
2. Compute e'th roots modulo p and q (easy)

Unproven that any solution to RSA implies factoring, but is the state of the art

*** How NOT to improve RSA performance
Use small private key d (order 2^128)

c^d = m mod N

means fewer exponentiations

**** Wiener's attack
Wiener 1987 hows if d < N^(1/4), then RSA is extremely insecure.

Remainder of video shows a lot of equation manipulation that results in
broken RSA if d is small.

** RSA in practice
To speed up RSA encryption, use a small e: c = m ^ e mod n
Minimum value is e=3, since we must have gcd(e, phi(N)) == 1
Recommended value is e = 65537 = 2^16 + 1, which requires 17 multiplications for encryptions

Asymmetry of RSA: fast encryption, slow decryption

**** Implementaiton attacks
Kocher 97: timing attack looking at the time it takes to compute c^d mod N to
expose d

Kocher 99: power attack: the power consumption of a smartcart while it is
computing c^d (mod N) can expose d

BDL 97 faults attack: a computer error during c^d (mod N) can expose d. Just 1
error is sufficient during decryption to expose d. A common defense is to
re-encrypt after decrypting, to make sure it matches the cipher. Since
encryption is 10x faster than decryption, this is a 10% slowdown.

**** Fault attack details
decrypt mod p: x_p = c^d in Z_p
decrypt mod q: x_q = c^d in Z_q
Combine to get x = c^d in Z_n

using chinese remainder theorem, which gives a ~4x speedup.

If an error occurs on x_q but not x_p, then we'll get:
x' = c^d in Z_p but x' != c^d in Z_q

which now means gcd((x')^e - c, N) = p in Z_q, which means I have the
factorization of N, which lets me easily compute phi(N), which lets me
immediately compute the private key.

** ElGamal Public Key System
Used in GPG, based on the Diffie-Hellman protocol

Fix a finite cyclic group G of order n
Fix a generator g in G

Alice choses A = g^a, for a  for a random a in {1,...,n}
Alice publishes A as a public key

Bob chooses a random b in {1,...,n} and computes g^(ab) = A^b
to derive the symmetric key k.

Bob sends ct = [ B = g^b, encrypt message m with k]

Alice can decrypt by computing B^a = k and decrypting the ct.

*** Formally
G: Finite cyclic gorup of order n
(E_s, D_s): symmetric auth. encryption defined over (K,M,C)
H: G^2 -> K a hash function

We construct a pub-key system (Gen, E, D):

Key Gen: choose random generator g in G, and a random a in Z_n
Output sk = a, pk = (g, h=g^a)

E(pk=(g,h), m):
  b <- Z_n
  u <- g^b
  v <- h^b
  k <- H(u,v)
  c <- E_s(k, m)
  output (u,c)

D(sk=a, (u,c)):
  v <- u^a
  k <- H(u,v)
  m <- D_s(k,c)
  output m

*** Performance
Encryption is 2 expoonentials:
b <- Z_n, u <- g^b, v <- h^b

Deryption requires one: v <- u^a

u always changes, so decryption can't be improved, but in encryption we can
pre-compute [ g^(2^i), h^(2^i) for i=1,...,log(n) ] and get a 3x speedup,
provided we're repeatedly encrypting to the same public key.

** ElGamal security
Requires that given g, g^a, g^b, it is difficult to compute g^ab

Computational Diffie-Hellman Assumption (CDH)

Hash Diffie-Hellman Assumption:
G: finite cyclic group of order n
H: G^2 -> K a nash function

Def: Hash-DH (HDH) assumption holds for (G,H) if:
(g,g^a,g^b, H(g^b, g^(ab)) is equiv to (g, g^a, g^b, R)
where g <- { generators of G}, a,b <- Z_n, R <- K

ElGamal is semantically secure under Hash-DH

We can prove chosen-ciphertext security with the Interactive DH assumption
with a random oracle hash function

*** With better security analysis
We can use a group G where CDH = IDH (bilinear group), which involves some
elliptic curve groups.

We can also change to Twin ElGamal variation:
KeyGen: g <- {generators of G}, a1, a2 <- Z_n

output pk = (g, h1 = g^a1, h2 = g^a2), sk = (a1,a2)

E(pk=(g,h1,h2), m):
  b <- Z_n
  k <- H(g^b, h1^b, h2^b)
  c <- E_s(k,m)
  output (g^b, c)

D(sk=(a1,a2), (u,c)):
  k <- H(u, u^a1, u^a2)
  m <- D_s(k,c)
  output m

This construct is provably CCA secure based only on the CDH assumption.
Unknown if this is really worth it or not.

*** ElGamal security without a random oracle hash function
Very active area of research in the last decade.

CHK'04 and BB'04 use bilinear groups with special elliptic curves.

CS'98 shows a construction with the Decision-DH assumption

** A Unifying Theme
Both RSA's trapdoors and Diffie-Hellman follow from one-way functions.

f: X -> Y is one-way if:
 - There is an efficient algorithm to evaluate f(.),
 - Computing f^(-1) is hard: for all efficient A and x <- X,
   Pr [ A(f(x)) ] < negligible, i.e. f(A(f(x))) != f(x)

In general, public key crypto requires one-way functions with special
homomorphic properties and things like trapdoors.

*** Generic one-way funcitons
Let f: X -> Y be a secure PRG, where |Y| >> |X|, e.g. f built using
deterministic counter mode.

Lemma: If f is a secure PRG, then f is one-way.

No special properties, so not suited for crypto.

*** The DLOG one-way function
Fix a finite cyclic gorup G (e.g. G = Z_p*) of order n.
Let g be a random generator in G

Define: f: Z_n -> G as f(x) = g^x in G

Lemma: If DLOG is hard in G, then f is one-way.

Additive Property: Given f(x), f(y), we can compute f(x+y) without having x or y
by f(x+y) = f(x)*f(y) = g^(x+y), which enables public key cryptography

*** The RSA one-way function
Choose random primes p,q =~ 1024 bits, set N=pq
Choose integers e,d s.t. e*d = 1 (mod phi(N))

Define: f: Z_n* -> Z_n* as f(x) = x ^ e in Z_n

Lemma: f is one-way under the RSA assumption

Multiplicative Property: f(x*y) = f(x) * f(y), and f has a trapdoor that lets us
invert the function with a secret key.

** Summary of material covered
Primitives:
PRGs, stream ciphers
PRF, PRP, with CTR mode and GGM construction to make block ciphers from PRGs

MACs like CMAC, HMAC, PMAC, and collision resistance

Authenticated encryption

Key exchange, pub key crypto, trapdoor functions and Diffie-Hellman groups

*** Remaining topics in part II
- Digital signatures and certs
- Authenticated key exchange
- User authentication: pw, one-time pw, challenge-response
- Privacy mechanisms
- Zero-knowledge protocols

*** Futher reading
- Elliptic curve crypto
- Quantum computing
- New key management paradigms
- Anonymous digital cash
- Private voting and auction
- Fully homomorphic encryption
- Lattice-based crypto
- Two party and multi-party computation
